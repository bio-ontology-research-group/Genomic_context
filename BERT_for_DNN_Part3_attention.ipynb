{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30151455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-12 11:31:13,465] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0258509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if CUDA is available and choose device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74339501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 12 11:31:31 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000000:89:00.0 Off |                    0 |\r\n",
      "| N/A   35C    P0              58W / 300W |    497MiB / 32768MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A   2692077      C   /home/toibazd/miniconda3/bin/python         494MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01aef35e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/toibazd/Data/BERT/BERT_context_pretrained_InterPro_final and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(544998, 256, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 256)\n",
       "    (token_type_embeddings): Embedding(2, 256)\n",
       "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): None\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"/home/toibazd/Data/BERT/BERT_context_pretrained_InterPro_final\"\n",
    "\n",
    "model = BertModel.from_pretrained(model_path)\n",
    "model.pooler = None\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df36f295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544998"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "tokenizer_path = \"WordLevel_tokenizer_trained_InterPro.json\"\n",
    "tokenizer = tokenizer.from_file(tokenizer_path)\n",
    "# tokenizer.enable_truncation(512)\n",
    "tokenizer.get_vocab_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5651a9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0203857421875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7767d86cd07c4dd5b879b43709b83ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140359\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import csv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "data_dict= defaultdict()\n",
    "mlb = MultiLabelBinarizer()\n",
    "row_num = 0\n",
    "with open(\"/home/toibazd/Prot2IP_GO_filtered_MF.tsv\", \"r\") as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter = \"\\t\")\n",
    "    for row in tqdm(reader):\n",
    "        row_num+=1\n",
    "        key = row[0].split(\"prot_\")[1].split(\".\")[0]\n",
    "        iprs = eval(row[1])\n",
    "        data_dict[key] = iprs\n",
    "print(row_num)\n",
    "one_hot_encoded = mlb.fit_transform(data_dict.values())\n",
    "one_hot_encoded_dict = {key:value for key, value in zip(data_dict.keys(), one_hot_encoded)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee7d270f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139055\n"
     ]
    }
   ],
   "source": [
    "print(len(one_hot_encoded_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23bbc1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "\n",
    "# directory = '/ibex/user/toibazd/InterPro_annotated_genomes/'\n",
    "# one_hot_encoded_sentences = {}\n",
    "\n",
    "# sentence_length = 40\n",
    "# num_files = 1\n",
    "# sentences_per_key = 1\n",
    "\n",
    "# for key in tqdm(one_hot_encoded_dict):\n",
    "#     if key not in one_hot_encoded_sentences:\n",
    "#         one_hot_encoded_sentences[key] = []\n",
    "\n",
    "#     file_list = os.listdir(directory)\n",
    "#     random.shuffle(file_list)\n",
    "#     sentences_count = 0\n",
    "\n",
    "\n",
    "#     for filename in file_list[:num_files]:\n",
    "#         filepath = os.path.join(directory, filename)\n",
    "\n",
    "#         with open(filepath, 'r') as file:\n",
    "#             content = file.read()\n",
    "#             words = content.strip().split()\n",
    "\n",
    "#             # Check if the content has enough words\n",
    "#             if len(words) >= sentence_length:\n",
    "#                 for i in range(len(words) - sentence_length + 1):\n",
    "#                     # Get the key from the word at position 20\n",
    "#                     current_key = words[i + 19]\n",
    "\n",
    "#                     # Check if the current key matches the desired key\n",
    "#                     if current_key == key:\n",
    "#                         # Create a sentence\n",
    "#                         sentence = \" \".join(words[i:i + sentence_length])\n",
    "\n",
    "#                         # Append the sentence to the list of sentences for the key\n",
    "#                         one_hot_encoded_sentences[key].append(sentence)\n",
    "#                         sentences_count += 1\n",
    "\n",
    "#                         # Break if the required number of sentences per key is reached\n",
    "#                         if sentences_count >= sentences_per_key:\n",
    "#                             break\n",
    "\n",
    "                            \n",
    "#             if sentences_count >= sentences_per_key:\n",
    "#                 break\n",
    "\n",
    "#         if sentences_count >= sentences_per_key:\n",
    "#             break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb37c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('BERT_DNN_senteces.json', \"r\") as f:\n",
    "    one_hot_encoded_sentences = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e5465cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_string = one_hot_encoded_sentences.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85633f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139055"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matching_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7a07816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1184096"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_string = [sentence for sublist in matching_string for sentence in sublist]\n",
    "len(matching_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ebd82ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3402\n"
     ]
    }
   ],
   "source": [
    "print(len(list(one_hot_encoded_dict.values())[1].flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9bc9984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "decision = True\n",
    "\n",
    "for sentence in matching_string:\n",
    "    words = sentence.split()\n",
    "    if words[19] not in one_hot_encoded_dict:\n",
    "        decision = False\n",
    "        print(sentence)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6faf847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in matching_string:\n",
    "    if len(i.split()) != 40:\n",
    "        print(\"error\")\n",
    "        print(len(i.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d2adf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184096\n"
     ]
    }
   ],
   "source": [
    "print(len(matching_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the labels\n",
    "labels_list = []\n",
    "\n",
    "# Iterate over all sentences in your list\n",
    "for sentence in sentences_list:\n",
    "    # Split the sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Ensure the sentence has at least 20 words\n",
    "    if len(words) >= 20:\n",
    "        # Grab the word at position 20 (index 19)\n",
    "        word_at_position_20 = words[19]\n",
    "\n",
    "        # Extract the label from one_hot_encoded_dict using the word as the key\n",
    "        if word_at_position_20 in one_hot_encoded_dict:\n",
    "            label = one_hot_encoded_dict[word_at_position_20]\n",
    "\n",
    "            # Append the label to the labels_list\n",
    "            labels_list.append(label)\n",
    "        else:\n",
    "            # Handle the case where the word is not found in one_hot_encoded_dict\n",
    "            # You may choose to handle this case differently based on your requirements\n",
    "            labels_list.append(None)\n",
    "    else:\n",
    "        # Handle the case where the sentence has less than 20 words\n",
    "        # You may choose to handle this case differently based on your requirements\n",
    "        labels_list.append(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2e51587",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example = matching_string[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eeedbd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs.pad(256,direction = 'right',pad_id = 3, pad_token = '[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50699cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     1,  36645,    263, 277097,  93666,  63739,   1197,    170,   2795,\n",
      "          48637,  83247,  14804,   2778, 305886, 311648,  33348,  26362,  82639,\n",
      "         290455, 292293, 178951,    676, 243814,  10212, 221617,  85358,   4287,\n",
      "         202923,  99402,  27999, 102888, 106238,    458,   6204, 258243, 312438,\n",
      "          32247, 301843, 303651, 102521, 320336,      2]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(one_example)\n",
    "model.to(\"cuda\")\n",
    "input_ids = torch.tensor([inputs.ids]).to(device)\n",
    "attention_mask = torch.tensor([inputs.attention_mask]).to(device)\n",
    "print(input_ids)\n",
    "print(attention_mask)\n",
    "# Generate the embeddings\n",
    "with torch.inference_mode():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states = True)\n",
    "\n",
    "hidden_states = outputs.last_hidden_state[0][20]\n",
    "#     mean_tensor = torch.mean(hidden_states[1:-1], dim=0)\n",
    "# embeddings_all_contexts.append(hidden_states.cpu())\n",
    "    \n",
    "# len(embeddings_all_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "073ba2e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01744985580444336,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1184096,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39aeace1c8e49c99339e7a0e245e256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1184096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 40, 40])\n",
      "tensor([[[[0.0281, 0.0276, 0.0267,  ..., 0.0248, 0.0248, 0.0248],\n",
      "          [0.0275, 0.0265, 0.0270,  ..., 0.0248, 0.0248, 0.0248],\n",
      "          [0.0254, 0.0253, 0.0258,  ..., 0.0249, 0.0249, 0.0249],\n",
      "          ...,\n",
      "          [0.0249, 0.0249, 0.0249,  ..., 0.0261, 0.0254, 0.0256],\n",
      "          [0.0249, 0.0249, 0.0249,  ..., 0.0264, 0.0261, 0.0265],\n",
      "          [0.0249, 0.0249, 0.0249,  ..., 0.0263, 0.0259, 0.0265]],\n",
      "\n",
      "         [[0.0251, 0.0251, 0.0252,  ..., 0.0250, 0.0250, 0.0250],\n",
      "          [0.0251, 0.0254, 0.0253,  ..., 0.0250, 0.0250, 0.0250],\n",
      "          [0.0250, 0.0250, 0.0251,  ..., 0.0250, 0.0250, 0.0250],\n",
      "          ...,\n",
      "          [0.0250, 0.0250, 0.0250,  ..., 0.0256, 0.0251, 0.0251],\n",
      "          [0.0250, 0.0250, 0.0250,  ..., 0.0252, 0.0252, 0.0251],\n",
      "          [0.0250, 0.0250, 0.0250,  ..., 0.0251, 0.0251, 0.0251]],\n",
      "\n",
      "         [[0.0250, 0.0251, 0.0255,  ..., 0.0250, 0.0250, 0.0250],\n",
      "          [0.0250, 0.0251, 0.0252,  ..., 0.0250, 0.0250, 0.0250],\n",
      "          [0.0251, 0.0251, 0.0250,  ..., 0.0250, 0.0250, 0.0250],\n",
      "          ...,\n",
      "          [0.0249, 0.0249, 0.0249,  ..., 0.0251, 0.0250, 0.0253],\n",
      "          [0.0250, 0.0250, 0.0250,  ..., 0.0251, 0.0250, 0.0250],\n",
      "          [0.0250, 0.0250, 0.0250,  ..., 0.0256, 0.0250, 0.0250]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0250, 0.0250, 0.0258,  ..., 0.0250, 0.0250, 0.0250],\n",
      "          [0.0250, 0.0250, 0.0252,  ..., 0.0250, 0.0250, 0.0250],\n",
      "          [0.0251, 0.0250, 0.0250,  ..., 0.0250, 0.0250, 0.0250],\n",
      "          ...,\n",
      "          [0.0250, 0.0250, 0.0250,  ..., 0.0251, 0.0250, 0.0252],\n",
      "          [0.0250, 0.0250, 0.0250,  ..., 0.0251, 0.0250, 0.0250],\n",
      "          [0.0250, 0.0250, 0.0250,  ..., 0.0254, 0.0250, 0.0250]],\n",
      "\n",
      "         [[0.0251, 0.0252, 0.0251,  ..., 0.0250, 0.0250, 0.0250],\n",
      "          [0.0251, 0.0257, 0.0253,  ..., 0.0250, 0.0250, 0.0250],\n",
      "          [0.0250, 0.0252, 0.0252,  ..., 0.0250, 0.0250, 0.0250],\n",
      "          ...,\n",
      "          [0.0250, 0.0250, 0.0250,  ..., 0.0255, 0.0252, 0.0250],\n",
      "          [0.0250, 0.0250, 0.0250,  ..., 0.0253, 0.0256, 0.0251],\n",
      "          [0.0250, 0.0250, 0.0250,  ..., 0.0251, 0.0252, 0.0251]],\n",
      "\n",
      "         [[0.0250, 0.0250, 0.0251,  ..., 0.0250, 0.0250, 0.0250],\n",
      "          [0.0250, 0.0250, 0.0251,  ..., 0.0250, 0.0250, 0.0250],\n",
      "          [0.0251, 0.0250, 0.0250,  ..., 0.0250, 0.0250, 0.0250],\n",
      "          ...,\n",
      "          [0.0250, 0.0250, 0.0250,  ..., 0.0250, 0.0250, 0.0253],\n",
      "          [0.0250, 0.0250, 0.0250,  ..., 0.0250, 0.0250, 0.0250],\n",
      "          [0.0250, 0.0250, 0.0250,  ..., 0.0252, 0.0250, 0.0250]]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The attention tensor does not have the correct number of dimensions. Make sure you set output_attentions=True when initializing your model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(softmax_attention)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Now you can visualize the softmax attention using model_view\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mmodel_view\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoftmax_attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Append hidden states and labels to the list\u001b[39;00m\n\u001b[1;32m     34\u001b[0m hidden_states_all\u001b[38;5;241m.\u001b[39mappend((hidden_states, one_example\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m19\u001b[39m]))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/bertviz/model_view.py:67\u001b[0m, in \u001b[0;36mmodel_view\u001b[0;34m(attention, tokens, sentence_b_start, prettify_tokens, display_mode, encoder_attention, decoder_attention, cross_attention, encoder_tokens, decoder_tokens, include_layers, include_heads, html_action)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_heads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     include_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(n_heads))\n\u001b[0;32m---> 67\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[43mformat_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_heads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentence_b_start \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     attn_data\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     70\u001b[0m         {\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m         }\n\u001b[1;32m     76\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/bertviz/util.py:11\u001b[0m, in \u001b[0;36mformat_attention\u001b[0;34m(attention, layers, heads)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_attention \u001b[38;5;129;01min\u001b[39;00m attention:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# 1 x num_heads x seq_len x seq_len\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_attention\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m---> 11\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe attention tensor does not have the correct number of dimensions. Make sure you set \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions=True when initializing your model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     layer_attention \u001b[38;5;241m=\u001b[39m layer_attention\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m heads:\n",
      "\u001b[0;31mValueError\u001b[0m: The attention tensor does not have the correct number of dimensions. Make sure you set output_attentions=True when initializing your model."
     ]
    }
   ],
   "source": [
    "from bertviz import model_view\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "hidden_states_all = []\n",
    "count = 0\n",
    "\n",
    "for one_example in tqdm(matching_string):\n",
    "    if count == 1:\n",
    "        break\n",
    "    inputs = tokenizer.encode(one_example)\n",
    "\n",
    "    input_ids = torch.tensor([inputs.ids]).to(device)\n",
    "    attention_mask = torch.tensor([inputs.attention_mask]).to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "    # Extract the attentions and hidden states\n",
    "    extracted_attentions = outputs.attentions[-1]\n",
    "    hidden_states = outputs.last_hidden_state[0][20]\n",
    "    \n",
    "    # Iterate through each layer's attention matrix\n",
    "\n",
    "    extracted_tensors = extracted_attentions[..., 1:-1, 1:-1]\n",
    "\n",
    "        # Apply softmax to the remaining attention weights for normalization\n",
    "    softmax_attention = torch.softmax(extracted_tensors, dim=-1)\n",
    "    print(softmax_attention.shape)\n",
    "    print(softmax_attention)\n",
    "        # Now you can visualize the softmax attention using model_view\n",
    "    model_view(softmax_attention, tokens)\n",
    "    \n",
    "    # Append hidden states and labels to the list\n",
    "    hidden_states_all.append((hidden_states, one_example.split()[19]))\n",
    "\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_view(extracted_tensors, tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
