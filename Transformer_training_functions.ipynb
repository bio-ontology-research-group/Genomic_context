{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992575ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_token(sentence, tokenizer):\n",
    "    # Tokenize the sentence and get the non-padding tokens\n",
    "    tokens = tokenizer.encode_plus(sentence, add_special_tokens=True, return_tensors=\"pt\",max_length = 15, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    input_ids = tokens[\"input_ids\"][0]\n",
    "    non_padding_tokens = [token for token in input_ids if token != tokenizer.pad_token_id]\n",
    "    \n",
    "    # Determine the index of the word to mask\n",
    "    if len(non_padding_tokens) > 0:\n",
    "        middle_index = len(input_ids) // 2\n",
    "        if input_ids[middle_index] == tokenizer.pad_token_id:\n",
    "            mask_index = 1\n",
    "        else:\n",
    "            mask_index = middle_index\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # Mask the selected token and get its label\n",
    "    masked_tokens = input_ids.detach().clone()\n",
    "    masked_tokens[mask_index] = tokenizer.mask_token_id\n",
    "    label = input_ids[mask_index]\n",
    "    \n",
    "    # Convert the masked tokens and label back to strings\n",
    "    masked_sentence = tokenizer.decode(masked_tokens, skip_special_tokens=False)\n",
    "    label = tokenizer.decode([label], skip_special_tokens=False)\n",
    "\n",
    "    return masked_sentence, label\n",
    "\n",
    "def divide_into_sentences(input_string, sentence_length=10, step_size=1):\n",
    "    words = input_string.split()\n",
    "    sentences = []\n",
    "    for i in range(0, len(words) - sentence_length + 1, step_size):\n",
    "        sentences.append(\" \".join(words[i: i + sentence_length]))    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def split_data(data, train_ratio=0.8, validate_ratio=0.1, test_ratio=0.1, seed=123):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(data)\n",
    "    n = len(data)\n",
    "    train_end = int(train_ratio * n)\n",
    "    validate_end = int((train_ratio + validate_ratio) * n)\n",
    "    train_data = data[:train_end]\n",
    "    validate_data = data[train_end:validate_end]\n",
    "    test_data = data[validate_end:]\n",
    "    return train_data, validate_data, test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647272f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e321d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1fe74e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'AutoModelForMaskedLM' has no attribute 'from_scratch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load BERT model with new vocabulary\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoModelForMaskedLM\u001b[38;5;241m.\u001b[39mfrom_scratch(model_name, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(tokens))\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForMaskedLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'AutoModelForMaskedLM' has no attribute 'from_scratch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForMaskedLM, BertConfig\n",
    "import random\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# Load BERT model with new vocabulary\n",
    "config = AutoModelForMaskedLM.from_scratch(model_name, num_labels=len(tokens))\n",
    "model = BertForMaskedLM.from_pretrained(model_name, config=config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "all_sentences = []\n",
    "absolute_path_genomes = \"/Users/daulettoibazar/Desktop/Research/models_and_data/annotation_extended/\"\n",
    "path_genome_files = os.listdir(\"/Users/daulettoibazar/Desktop/Research/models_and_data/annotation_extended/\")\n",
    "\n",
    "for file_name in tqdm(path_genome_files):\n",
    "    with open(os.path.join(absolute_path_genomes, file_name), \"r\", encoding=\"latin_1\") as infile:\n",
    "        content = infile.read()\n",
    "        short_sentences = divide_into_sentences(content)\n",
    "        all_sentences+=short_sentences\n",
    "\n",
    "train_sentences, validation_sentences, test_sentences = split_data(all_sentences)\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, sentence in enumerate(train_sentences, 0):\n",
    "        masked_sentence, label  = mask_token(sentence)\n",
    "        \n",
    "        tokenized_text = masked_sentence.split()\n",
    "        masked_index = tokenized_text.index('[MASK]')\n",
    "        tokenized_text[masked_index] = '[MASK]'\n",
    "        \n",
    "        tokens = tokenizer.encode_plus(masked_sentence, add_special_tokens=False, return_tensors=\"pt\",max_length = 15, padding=\"max_length\", truncation=True)\n",
    "        input_ids = tokens[\"input_ids\"][0]\n",
    "        labels = tokenizer.encode_plus(masked_sentence, add_special_tokens=False, return_tensors=\"pt\",max_length = 15, padding=\"max_length\", truncation=True)\n",
    "        ids_to_check = labels[\"input_ids\"][0]\n",
    "        \n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, labels = ids_to_check, attention_mask = tokens[\"attention_mask\"][0])\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # Print every 1000 sentences\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da9fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sentence in validation_sentences:\n",
    "        masked_sentence, label  = mask_token(sentence)\n",
    "        \n",
    "        tokenized_text = masked_sentence.split()\n",
    "        masked_index = tokenized_text.index('[MASK]')\n",
    "        tokenized_text[masked_index] = '[MASK]'\n",
    "        \n",
    "        tokens = tokenizer.encode_plus(masked_sentence, add_special_tokens=False, return_tensors=\"pt\",max_length = 15, padding=\"max_length\", truncation=True)\n",
    "        input_ids = tokens[\"input_ids\"][0]\n",
    "        labels = tokenizer.encode_plus(masked_sentence, add_special_tokens=False, return_tensors=\"pt\",max_length = 15, padding=\"max_length\", truncation=True)\n",
    "        ids_to_check = labels[\"input_ids\"][0]\n",
    "        \n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits[0, masked_index, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        predicted_token_id = torch.argmax(logits, dim=-1)\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens(predicted_token_id)\n",
    "        target = tokenizer.convert_ids_to_tokens(ids_to_check)\n",
    "        # Count the number of correct predictions\n",
    "        total += 1\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    # Print the accuracy\n",
    "    accuracy = 100.0 * correct / total\n",
    "    print('Accuracy: %.2f %%' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691efc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sentence in test_sentences:\n",
    "        masked_sentence, label  = mask_token(sentence)\n",
    "        \n",
    "        tokenized_text = masked_sentence.split()\n",
    "        masked_index = tokenized_text.index('[MASK]')\n",
    "        tokenized_text[masked_index] = '[MASK]'\n",
    "        \n",
    "        tokens = tokenizer.encode_plus(masked_sentence, add_special_tokens=False, return_tensors=\"pt\",max_length = 15, padding=\"max_length\", truncation=True)\n",
    "        input_ids = tokens[\"input_ids\"][0]\n",
    "        labels = tokenizer.encode_plus(masked_sentence, add_special_tokens=False, return_tensors=\"pt\",max_length = 15, padding=\"max_length\", truncation=True)\n",
    "        ids_to_check = labels[\"input_ids\"][0]\n",
    "        \n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits[0, masked_index, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        predicted_token_id = torch.argmax(logits, dim=-1)\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens(predicted_token_id)\n",
    "        target = tokenizer.convert_ids_to_tokens(ids_to_check)\n",
    "        # Count the number of correct predictions\n",
    "        total += 1\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    # Print the accuracy\n",
    "    accuracy = 100.0 * correct / total\n",
    "    print('Accuracy: %.2f %%' % accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
