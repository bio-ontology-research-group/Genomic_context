{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c622ad12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free CPU Memory: 258.61 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def get_free_memory():\n",
    "    memory = psutil.virtual_memory()\n",
    "    return memory.available / (1024.0 ** 3)  # Convert bytes to gigabytes\n",
    "\n",
    "print(f\"Free CPU Memory: {get_free_memory():.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abdd98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "from datasets import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4afee737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = WhitespaceSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d69a29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"WordLevel_tokenizer_trained_InterPro.json\"\n",
    "tokenizer = tokenizer.from_file(tokenizer_path)\n",
    "tokenizer.enable_truncation(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bef36e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544998"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "242995ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2966, 754, 2545, 196, 9231, 2817, 7418, 2526, 2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tokenizer.encode(\"WP_265490204 WP_206642677 WP_053312998 WP_251959347 WP_000076573 WP_227526754 WP_218401808 WP_106925592\")\n",
    "test.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a5c8709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIT_IP.tsv\n",
      "CRISPR_IP.tsv\n",
      "BREX_IP.tsv\n",
      "DISARM_IP.tsv\n",
      "AbiH_IP.tsv\n",
      "Kiwa_IP.tsv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "defense_ips = []\n",
    "directory = \"/home/toibazd/Defense_InterPros/\"\n",
    "file_list = os.listdir(directory)\n",
    "\n",
    "for file in file_list:\n",
    "    print(file)\n",
    "    file_path = os.path.join(directory, file)\n",
    "    with open(file_path, 'r', newline='') as infile:\n",
    "        reader = csv.reader(infile, delimiter = \"\\t\")\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            ip = row[0]\n",
    "            defense_ips.append(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "679dec11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defense IPs number:  120\n"
     ]
    }
   ],
   "source": [
    "print(\"Defense IPs number: \", len(defense_ips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07ba9843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005791902542114258,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41b7eb3691848abaf81f69abd19ac70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "data_dict = defaultdict(list)\n",
    "\n",
    "with open(\"/home/toibazd/Prot2IP.tsv\", \"r\") as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter = \"\\t\")\n",
    "    for row in tqdm(reader):\n",
    "        key = row[0].split(\"prot_\")[1].split(\".\")[0]\n",
    "        iprs = eval(row[1])\n",
    "\n",
    "        # Save only if there are filtered InterPro IDs\n",
    "        for ip in iprs:\n",
    "            if ip in defense_ips:\n",
    "                data_dict[key].append(ip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "334d007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_defense = set()\n",
    "\n",
    "# Iterate through each value list in the dictionary and add its elements to the set\n",
    "for value_list in data_dict.values():\n",
    "    unique_defense.update(value_list)\n",
    "\n",
    "# Convert the set to a list if needed\n",
    "unique_defense = list(unique_defense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88b651fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_defense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83f471e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "enc = MultiLabelBinarizer()\n",
    "one_hot_encoded = enc.fit_transform(data_dict.values())\n",
    "one_hot_encoded_dict = {key: value for key, value in zip(data_dict.keys(), one_hot_encoded)}\n",
    "\n",
    "print(len(one_hot_encoded_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7464990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(661, 83)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a51e75ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# from tqdm import tqdm\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# directory = '/ibex/user/toibazd/InterPro_annotated_genomes/'\n",
    "# one_hot_encoded_sentences = {}\n",
    "\n",
    "# sentence_length = 40\n",
    "# sentences_per_IP = 100\n",
    "\n",
    "# # Set random seed for reproducibility\n",
    "# random.seed(42)\n",
    "\n",
    "# # Randomly choose 1000 files with seed 42\n",
    "# selected_files = os.listdir(directory)\n",
    "\n",
    "\n",
    "# # Define a function to process a file\n",
    "# def process_file(filename, IP):\n",
    "#     sentences = []\n",
    "\n",
    "#     filepath = os.path.join(directory, filename)\n",
    "\n",
    "#     with open(filepath, 'r') as file:\n",
    "#         content = file.read()\n",
    "#         words = content.strip().split()\n",
    "\n",
    "#         # Check if the key is in the file\n",
    "#         for i in range(19, len(words)-20):\n",
    "#             # Shuffle the indices of the words containing the key\n",
    "#             if IP in data_dict[words[i]]:\n",
    "#                 if len(words) - i >= 21:\n",
    "#                     sentence = \" \".join(words[i - 19:i + sentence_length - 19])\n",
    "#                     sentences.append(sentence)\n",
    "#     return sentences\n",
    "\n",
    "\n",
    "# # Iterate over keys\n",
    "# for IP in tqdm(unique_defense):\n",
    "#     one_hot_encoded_sentences[IP] = []\n",
    "#     sentences_count = 0\n",
    "\n",
    "#     # Use ThreadPoolExecutor for concurrent processing\n",
    "#     with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "#         futures = [executor.submit(process_file, filename, IP) for filename in selected_files]\n",
    "#         for future in futures:\n",
    "#             sentences = future.result()\n",
    "#             one_hot_encoded_sentences[IP].extend(sentences)\n",
    "#             sentences_count += len(sentences)\n",
    "#             if sentences_count >= sentences_per_IP:\n",
    "#                 break\n",
    "\n",
    "#     # Break if the required number of sentences per key is reached\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abea8bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, values in one_hot_encoded_sentences.items():\n",
    "#     print(len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a55b4b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('BERT_sanity_check_defense_DNN_senteces.json', 'w') as f:\n",
    "#     json.dump(one_hot_encoded_sentences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eeeb70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('BERT_sanity_check_defense_DNN_senteces.json', \"r\") as f:\n",
    "    one_hot_encoded_sentences = json.load(f)\n",
    "    \n",
    "    \n",
    "with open('BERT_sanity_check_defense_DNN_senteces_testing.json', \"r\") as f:\n",
    "    test_sentences = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1149a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPR013381 100\n",
      "IPR030955 19\n",
      "IPR013487 18\n",
      "IPR028629 106\n",
      "IPR047679 100\n",
      "IPR013410 51\n",
      "IPR048067 71\n",
      "IPR010154 100\n",
      "IPR047721 100\n",
      "IPR047939 100\n",
      "IPR010147 100\n",
      "IPR010144 100\n",
      "IPR017576 19\n",
      "IPR013421 59\n",
      "IPR013489 100\n",
      "IPR017575 33\n",
      "IPR010160 14\n",
      "IPR027620 25\n",
      "IPR019089 77\n",
      "IPR014174 100\n",
      "IPR021124 100\n",
      "IPR010152 100\n",
      "IPR010172 48\n",
      "IPR049832 42\n",
      "IPR005537 101\n",
      "IPR013413 57\n",
      "IPR010180 100\n",
      "IPR049889 36\n",
      "IPR019199 100\n",
      "IPR013414 100\n",
      "IPR010156 100\n",
      "IPR013403 73\n",
      "IPR013408 100\n",
      "IPR002729 102\n",
      "IPR013415 28\n",
      "IPR017574 56\n",
      "IPR013419 65\n",
      "IPR010179 100\n",
      "IPR047583 26\n",
      "IPR027616 34\n",
      "IPR049794 17\n",
      "IPR017589 33\n",
      "IPR010149 100\n",
      "IPR013490 19\n",
      "IPR019857 100\n",
      "IPR013492 100\n",
      "IPR013396 100\n",
      "IPR019858 100\n",
      "IPR021127 100\n",
      "IPR006482 100\n",
      "IPR033641 100\n",
      "IPR032359 100\n",
      "IPR013418 100\n",
      "IPR010148 101\n",
      "IPR019855 100\n",
      "IPR019851 100\n",
      "IPR013444 20\n",
      "IPR013343 100\n",
      "IPR013395 100\n",
      "IPR013398 100\n",
      "IPR031820 84\n",
      "IPR013397 100\n",
      "IPR019504 100\n",
      "IPR013412 100\n",
      "IPR010155 100\n",
      "IPR013382 100\n",
      "IPR014858 100\n",
      "IPR019092 57\n",
      "IPR023843 69\n",
      "IPR013399 100\n",
      "IPR021228 100\n",
      "IPR005510 100\n",
      "IPR010146 100\n",
      "IPR025935 100\n",
      "IPR013407 34\n",
      "IPR023844 49\n",
      "IPR013337 100\n",
      "IPR010173 100\n",
      "IPR049758 33\n",
      "IPR016581 27\n",
      "IPR027617 35\n",
      "IPR019117 16\n",
      "IPR019856 100\n"
     ]
    }
   ],
   "source": [
    "for key, value in one_hot_encoded_sentences.items():\n",
    "    print(key, end=\" \")\n",
    "    print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deeb72f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of one_hot_encoded sentences items  83\n",
      "Len of one_hot_encoded_sentences values  83\n",
      "Len of all sentences in matching strings:  6354\n"
     ]
    }
   ],
   "source": [
    "one_hot_encoded_sentences = {key: value for key, value in one_hot_encoded_sentences.items() if value}\n",
    "print(\"Len of one_hot_encoded sentences items \",len(one_hot_encoded_sentences))\n",
    "\n",
    "matching_string = one_hot_encoded_sentences.values()\n",
    "print(\"Len of one_hot_encoded_sentences values \", len(matching_string))\n",
    "\n",
    "matching_string = [item for sublist in matching_string for item in sublist]\n",
    "print(\"Len of all sentences in matching strings: \", len(matching_string))\n",
    "\n",
    "train_words_list = [sentence.split() for sentence in matching_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df8e127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of test sentences items 83\n",
      "Len of test sentences values 83\n",
      "Len of test sentences in matching strings:  2659\n"
     ]
    }
   ],
   "source": [
    "test_sentences = {key: value for key, value in test_sentences.items() if value}\n",
    "print(\"Len of test sentences items\", len(test_sentences))\n",
    "test_matching_string = test_sentences.values()\n",
    "print(\"Len of test sentences values\", len(test_matching_string))\n",
    "test_matching_string = [item for sublist in test_matching_string for item in sublist]\n",
    "print(\"Len of test sentences in matching strings: \", len(test_matching_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aab0e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_words_list = [sentence.split() for sentence in test_matching_string]\n",
    "\n",
    "fit_word_list = train_words_list+test_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c1c4fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9013\n"
     ]
    }
   ],
   "source": [
    "count= 0\n",
    "for string in fit_word_list:\n",
    "    if not string[19] in data_dict.keys():\n",
    "        print(\"False\")\n",
    "    else:\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a45e9247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6354, 46045)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(fit_word_list)\n",
    "\n",
    "\n",
    "one_hot_contexts = mlb.transform(train_words_list)\n",
    "embeddings = one_hot_contexts.tolist()\n",
    "one_hot_contexts.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b43fe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/toibazd/Data/BERT/BERT_context_pretrained_InterPro_final and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(544998, 256, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 256)\n",
       "    (token_type_embeddings): Embedding(2, 256)\n",
       "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): None\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# Check if CUDA is available and choose device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = \"/home/toibazd/Data/BERT/BERT_context_pretrained_InterPro_final\"\n",
    "\n",
    "model = BertModel.from_pretrained(model_path).cuda()\n",
    "model.pooler = None\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "978dfa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83,)\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_encoded[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c35dd100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 31.65it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128 # Define your batch size\n",
    "model.cuda()\n",
    "ems = []\n",
    "labels = []\n",
    "\n",
    "# Define your data iterator in batches\n",
    "for i in tqdm(range(0, len(matching_string), batch_size)):\n",
    "    batch_sentences = matching_string[i:i+batch_size]\n",
    "\n",
    "    batch_inputs = tokenizer.encode_batch(batch_sentences)\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "\n",
    "    for encoding in batch_inputs:\n",
    "        input_ids_list.append(encoding.ids)\n",
    "        attention_mask_list.append(encoding.attention_mask)\n",
    "\n",
    "    # Convert lists to tensors and move to device\n",
    "    input_ids = torch.tensor(input_ids_list).cuda()\n",
    "    attention_mask = torch.tensor(attention_mask_list).cuda()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    relevant_hidden_states = hidden_states[:, 20, :]  # Extract the 20th token's hidden state\n",
    "    \n",
    "    for j in range(len(batch_sentences)):\n",
    "        ems.append(relevant_hidden_states[j].cpu())\n",
    "        indicator = batch_sentences[j].split()[19]\n",
    "        labels.append(one_hot_encoded_dict[indicator])\n",
    "\n",
    "# Ensure order in embeddings matches order in labels\n",
    "\n",
    "# Now embeddings and labels are stored on the CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cad879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pos_weights(class_counts):\n",
    "    pos_weights = np.ones_like(class_counts)\n",
    "    \n",
    "    neg_counts = [len(embeddings)-pos_count for pos_count in class_counts]\n",
    "    for cdx, (pos_count, neg_count) in enumerate(zip(class_counts,  neg_counts)):\n",
    "      pos_weights[cdx] = neg_count / (pos_count + 1e-5)\n",
    "\n",
    "    return torch.as_tensor(pos_weights, dtype=torch.float)\n",
    "class_counts = np.array(labels).sum(axis=0)\n",
    "pos_weights = calculate_pos_weights(class_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1476691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  6.,  55.,  13.,  22.,  62.,  62.,  32.,  38.,  62.,  62.,  26.,  54.,\n",
       "         62., 452., 116.,  47.,  62.,  62.,  61.,  41.,  62.,  50.,  62.,  62.,\n",
       "         62.,  62.,  62.,  86., 185.,  62., 118.,  43.,  54.,  30., 225.,  43.,\n",
       "         93., 106., 316., 351.,  62., 301.,  62.,  62.,  62., 146., 112., 191.,\n",
       "        333., 191.,  81.,  54., 396.,  30.,  62.,  28.,  58.,  60.,  58.,  59.,\n",
       "         14.,  46.,  62.,  91., 121.,  62., 185., 180., 253.,  58., 333.,  74.,\n",
       "         62.,  29., 243.,  62.,  62.,  62.,  88., 191., 372., 150., 175.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fc1f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "# Zip the lists together\n",
    "combined = list(zip(embeddings, labels))\n",
    "\n",
    "# Shuffle the combined list\n",
    "random.shuffle(combined)\n",
    "\n",
    "# Unzip the shuffled list\n",
    "embeddings, labels = zip(*combined)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7184dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f832c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Classification_V0(nn.Module):\n",
    "    def __init__(self, input_dim, first_hidden, second_hidden, last_hidden, output_dim, dropout_prob):\n",
    "        super(Classification_V0, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, first_hidden)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(first_hidden, second_hidden)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(second_hidden, last_hidden)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(last_hidden, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "input_dim = 46045\n",
    "first_hidden = 256\n",
    "second_hidden = 128\n",
    "last_hidden = 64\n",
    "output_dim = 83\n",
    "dropout_prob = 0.25\n",
    "\n",
    "clf_model = Classification_V0(input_dim, first_hidden, second_hidden, last_hidden, output_dim, dropout_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d9abc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "batch_size = 256\n",
    "def data_generator(embeddings, labels, batch_size):\n",
    "    num_samples = len(embeddings)\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_embeddings = embeddings[i:i+batch_size]\n",
    "        batch_labels = labels[i:i+batch_size]\n",
    "        yield batch_embeddings, batch_labels\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(clf_model.parameters(), lr=0.001)\n",
    "scheduler = lr_scheduler.StepLR(optimizer=optimizer, step_size=10, gamma=0.1)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad1b6ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3377521143869142\n",
      "Epoch 2/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0391231149031674\n",
      "Epoch 3/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6200997554021397\n",
      "Epoch 4/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41100516362801115\n",
      "Epoch 5/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32114361050671514\n",
      "Epoch 6/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27794225762991326\n",
      "Epoch 7/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2571619044019623\n",
      "Epoch 8/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23380076918993725\n",
      "Epoch 9/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2218091290515663\n",
      "Epoch 10/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21038077453190626\n",
      "Epoch 11/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20468899466010584\n",
      "Epoch 12/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20056108037067877\n",
      "Epoch 13/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19327611926219582\n",
      "Epoch 14/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1942177863476747\n",
      "Epoch 15/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1932064983354712\n",
      "Epoch 16/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19369991507813883\n",
      "Epoch 17/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19136352947308352\n",
      "Epoch 18/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18986978191679516\n",
      "Epoch 19/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18837865734010287\n",
      "Epoch 20/20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18881272774355046\n",
      "Training finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_epochs = 20\n",
    "epoch_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    \n",
    "    # Initialize data generator\n",
    "    generator = data_generator(embeddings, labels, batch_size)\n",
    "    train_loss = 0\n",
    "    # Iterate over batches\n",
    "    for batch_embeddings, batch_labels in tqdm(generator, desc=\"Training Batches\", leave=False):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_embeddings_tensor = torch.tensor(batch_embeddings, dtype= torch.float32)\n",
    "        batch_labels = np.array(batch_labels)\n",
    "        batch_labels_tensor = torch.tensor(batch_labels, dtype = torch.float32)\n",
    "        \n",
    "        outputs = clf_model(batch_embeddings_tensor)\n",
    "        loss = criterion(outputs, batch_labels_tensor)\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    epoch_loss.append(train_loss/(len(embeddings)/batch_size))\n",
    "    print(train_loss/(len(embeddings)/batch_size))\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac44af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory = '/ibex/user/toibazd/InterPro_annotated_genomes_for_testing/'\n",
    "# one_hot_encoded_sentences_2 = {}\n",
    "\n",
    "# sentence_length = 40\n",
    "# sentence_per_IP = 50\n",
    "# selected_files = os.listdir(directory)\n",
    "\n",
    "# total_sentences = sum(len(sentences) for sentences in one_hot_encoded_sentences.values())\n",
    "\n",
    "# # Iterate over keys\n",
    "# for IP in tqdm(unique_defense):\n",
    "#     one_hot_encoded_sentences_2[IP] = []\n",
    "#     sentences_count=0\n",
    "    \n",
    "#     # Iterate over selected files\n",
    "#     for filename in selected_files:\n",
    "#         filepath = os.path.join(directory, filename)\n",
    "\n",
    "#         with open(filepath, 'r') as file:\n",
    "#             content = file.read()\n",
    "#             words = content.strip().split()\n",
    "\n",
    "#             # Check if the key is in the file\n",
    "#             for i in range(19, len(words)-20):\n",
    "#                 # Shuffle the indices of the words containing the key\n",
    "#                 if IP in data_dict[words[i]]:\n",
    "#                     if len(words) - i >= 21:\n",
    "#                         sentence = \" \".join(words[i - 19:i + sentence_length - 19])\n",
    "#                         one_hot_encoded_sentences_2[IP].append(sentence)\n",
    "#                         sentences_count += 1\n",
    "#                         if sentences_count>=sentence_per_IP:\n",
    "#                             break\n",
    "#         if sentences_count>=sentence_per_IP:\n",
    "#             break\n",
    "#     print(sentences_count)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7873eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in one_hot_encoded_sentences_2.items():\n",
    "#     print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5a2eb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('BERT_sanity_check_defense_DNN_senteces_testing.json', 'w') as f:\n",
    "#     json.dump(one_hot_encoded_sentences_2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0aae8a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "with open('BERT_sanity_check_defense_DNN_senteces_testing.json', \"r\") as f:\n",
    "    test_sentences = json.load(f)\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a93cb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2659"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = {key: value for key, value in test_sentences.items() if value}\n",
    "print(len(test_sentences))\n",
    "test_matching_string = test_sentences.values()\n",
    "len(test_matching_string)\n",
    "test_matching_string = [item for sublist in test_matching_string for item in sublist]\n",
    "len(test_matching_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc8e268b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2659, 46045)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "words_list = [sentence.split() for sentence in test_matching_string]\n",
    "one_hot_contexts = mlb.transform(words_list)\n",
    "embeddings = one_hot_contexts.tolist()\n",
    "one_hot_contexts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d499e52",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:03<00:00,  5.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from bertviz import model_view\n",
    "\n",
    "batch_size = 128 # Define your batch size\n",
    "# model.cuda()\n",
    "test_ems = []\n",
    "test_labels = []\n",
    "\n",
    "# Define your data iterator in batches\n",
    "for i in tqdm(range(0, len(test_matching_string), batch_size)):\n",
    "    batch_sentences = test_matching_string[i:i+batch_size]\n",
    "    batch_inputs = tokenizer.encode_batch(batch_sentences)\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "\n",
    "    for encoding in batch_inputs:\n",
    "        input_ids_list.append(encoding.ids)\n",
    "        attention_mask_list.append(encoding.attention_mask)\n",
    "    # Convert lists to tensors and move to device\n",
    "    try:\n",
    "        input_ids = torch.tensor(input_ids_list)\n",
    "    except:\n",
    "        for ins in input_ids_list:\n",
    "            if len(ins)!=42:\n",
    "                print(len(ins))\n",
    "                print(ins)\n",
    "    attention_mask = torch.tensor(attention_mask_list)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True, output_attentions = True)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    attentions = outputs.attentions[-1]\n",
    "    relevant_hidden_states = hidden_states[:, 20, :]  # Extract the 20th token's hidden state\n",
    "#     for i in range(len(batch_sentences)):\n",
    "#         if i < 20:\n",
    "#             att = []\n",
    "#             extracted_tensor = attentions[i, :, :, :]\n",
    "#             extracted_tensor = extracted_tensor.unsqueeze(0)\n",
    "#             print(extracted_tensor.shape)\n",
    "#             att.append(extracted_tensor)\n",
    "\n",
    "#             tokens = \"[CLS] \"+tokenizer.decode(input_ids_list[i])+\" [SEP]\"\n",
    "#             tokens = tokens.split(\" \")\n",
    "#             model_view(att, tokens)\n",
    "    for j in range(len(batch_sentences)):\n",
    "        # Move embeddings to CPU\n",
    "        test_ems.append(relevant_hidden_states[j].cpu())\n",
    "        indicator = batch_sentences[j].split()[19]\n",
    "        test_labels.append(one_hot_encoded_dict[indicator])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00089282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WP_072018053 WP_072018053 WP_072018053 WP_072018053'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([0,12,12,12,12,\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b5d975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6194248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classification_V0(\n",
       "  (fc1): Linear(in_features=46045, out_features=256, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc4): Linear(in_features=64, out_features=83, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d150c202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                           \r"
     ]
    }
   ],
   "source": [
    "generator = data_generator(embeddings, test_labels, batch_size)\n",
    "# Iterate over batches\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "# Initialize lists to store predictions and labels across all batches\n",
    "# Iterate over batches\n",
    "count = 0\n",
    "for batch_embeddings, batch_labels in tqdm(generator, desc=\"Evaluation Batches\", leave=False):\n",
    "    batch_embeddings_tensor = torch.tensor(batch_embeddings, dtype= torch.float32)\n",
    "    batch_labels = np.array(batch_labels)\n",
    "    logits = clf_model(batch_embeddings_tensor)\n",
    "    predictions = torch.sigmoid(logits)\n",
    "    thresholded_predictions = (predictions > 0.85).float()\n",
    "    all_predictions.append(thresholded_predictions.detach().numpy())\n",
    "    all_labels.append(batch_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd80f04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_predictions[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2eb88b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_labels[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f0b4f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 83)\n"
     ]
    }
   ],
   "source": [
    "print(all_predictions[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe31ad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 83)\n"
     ]
    }
   ],
   "source": [
    "print(all_labels[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a17e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate predictions and labels across all batches\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "all_labels = np.concatenate(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0fb58693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2659, 83)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c72bd654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2659, 83)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "333465a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "cl_report = multilabel_confusion_matrix(all_labels, all_predictions)\n",
    "print(\"Classification report:\")\n",
    "print(len(cl_report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b75b6cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2247   22]\n",
      " [ 316   74]]\n"
     ]
    }
   ],
   "source": [
    "print(cl_report[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6bb7d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(all_labels, all_predictions, zero_division=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ecbf3a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 96., 204., 246., 187., 101.,  85., 180., 152., 232., 141., 154.,\n",
       "       175., 102.,  12.,  23., 214., 220., 113., 113.,  66., 148., 141.,\n",
       "       260., 264., 259., 264., 264.,  24.,  22., 177.,  32., 296.,   9.,\n",
       "       127.,  26., 169.,   6.,  49.,   8.,   5.,  91.,   2.,  56.,  52.,\n",
       "       118.,  24.,  38.,  22.,  17.,  23.,  24.,   9.,  12.,  70.,  32.,\n",
       "       149., 118., 174., 247.,  43., 154.,  67.,  36.,  38.,  38.,  49.,\n",
       "        29.,  29.,  29., 105.,   3.,  34.,  42., 148.,   2., 115.,  43.,\n",
       "       113.,  16.,   9.,   4.,  14.,   3.], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "692d99ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.19      0.30       390\n",
      "           1       0.25      0.86      0.38        58\n",
      "           2       0.63      0.82      0.71       190\n",
      "           3       0.37      0.67      0.48       105\n",
      "           4       0.38      0.76      0.50        50\n",
      "           5       0.55      0.94      0.70        50\n",
      "           6       0.37      0.75      0.49        88\n",
      "           7       0.39      0.80      0.53        75\n",
      "           8       0.20      0.94      0.33        50\n",
      "           9       0.30      0.86      0.45        50\n",
      "          10       0.49      0.80      0.60        94\n",
      "          11       0.23      0.69      0.34        58\n",
      "          12       0.28      0.58      0.38        50\n",
      "          13       0.42      1.00      0.59         5\n",
      "          14       0.39      1.00      0.56         9\n",
      "          15       0.31      1.00      0.47        66\n",
      "          16       0.19      0.82      0.30        50\n",
      "          17       0.19      1.00      0.31        21\n",
      "          18       0.22      0.93      0.36        27\n",
      "          19       0.27      0.30      0.28        61\n",
      "          20       0.28      0.84      0.42        50\n",
      "          21       0.36      0.88      0.51        58\n",
      "          22       0.17      0.90      0.29        50\n",
      "          23       0.17      0.90      0.29        50\n",
      "          24       0.18      0.92      0.30        50\n",
      "          25       0.17      0.88      0.28        50\n",
      "          26       0.16      0.86      0.27        50\n",
      "          27       0.46      0.85      0.59        13\n",
      "          28       0.23      0.83      0.36         6\n",
      "          29       0.24      0.84      0.37        50\n",
      "          30       0.38      0.86      0.52        14\n",
      "          31       0.19      0.77      0.30        73\n",
      "          32       0.89      0.50      0.64        16\n",
      "          33       0.44      0.73      0.55        77\n",
      "          34       0.15      1.00      0.27         4\n",
      "          35       0.28      0.65      0.40        74\n",
      "          36       1.00      1.00      1.00         6\n",
      "          37       0.43      0.91      0.58        23\n",
      "          38       0.25      1.00      0.40         2\n",
      "          39       0.40      1.00      0.57         2\n",
      "          40       0.46      0.84      0.60        50\n",
      "          41       0.50      0.25      0.33         4\n",
      "          42       0.89      1.00      0.94        50\n",
      "          43       0.52      0.87      0.65        31\n",
      "          44       0.32      0.76      0.45        50\n",
      "          45       0.46      1.00      0.63        11\n",
      "          46       0.26      1.00      0.42        10\n",
      "          47       0.14      0.75      0.23         4\n",
      "          48       0.06      1.00      0.11         1\n",
      "          49       0.13      0.75      0.22         4\n",
      "          50       0.38      0.75      0.50        12\n",
      "          51       0.89      0.50      0.64        16\n",
      "          52       0.25      1.00      0.40         3\n",
      "          53       0.59      0.41      0.48       100\n",
      "          54       0.94      0.94      0.94        32\n",
      "          55       0.51      0.71      0.59       107\n",
      "          56       0.36      0.77      0.49        56\n",
      "          57       0.20      0.66      0.31        53\n",
      "          58       0.16      0.80      0.27        50\n",
      "          59       0.88      0.72      0.79        53\n",
      "          60       0.56      0.49      0.52       177\n",
      "          61       0.22      0.22      0.22        69\n",
      "          62       0.83      0.60      0.70        50\n",
      "          63       0.34      0.93      0.50        14\n",
      "          64       0.32      0.86      0.46        14\n",
      "          65       0.98      0.96      0.97        50\n",
      "          66       0.34      1.00      0.51        10\n",
      "          67       0.34      1.00      0.51        10\n",
      "          68       0.31      1.00      0.47         9\n",
      "          69       0.35      0.74      0.48        50\n",
      "          70       1.00      1.00      1.00         3\n",
      "          71       0.79      1.00      0.89        27\n",
      "          72       0.98      0.82      0.89        50\n",
      "          73       0.50      0.72      0.59       103\n",
      "          74       0.50      1.00      0.67         1\n",
      "          75       0.33      0.76      0.46        50\n",
      "          76       1.00      0.86      0.92        50\n",
      "          77       0.33      0.74      0.45        50\n",
      "          78       1.00      1.00      1.00        16\n",
      "          79       1.00      1.00      1.00         9\n",
      "          80       1.00      1.00      1.00         4\n",
      "          81       0.50      1.00      0.67         7\n",
      "          82       0.33      1.00      0.50         1\n",
      "\n",
      "   micro avg       0.34      0.70      0.46      3786\n",
      "   macro avg       0.44      0.82      0.52      3786\n",
      "weighted avg       0.47      0.70      0.49      3786\n",
      " samples avg       0.47      0.74      0.48      3786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a64d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enc.classes_)[36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e153aae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "num_classes = 83\n",
    "# Compute precision, recall, and thresholds for each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "thresholds = dict()\n",
    "average_precision = dict()\n",
    "\n",
    "for i in range(num_classes):  # num_classes is the number of classes\n",
    "    precision[i], recall[i], thresholds[i] = precision_recall_curve(all_labels[:, i], all_predictions[:, i])\n",
    "    average_precision[i] = average_precision_score(all_labels[:, i], all_predictions[:, i])\n",
    "\n",
    "# Compute micro-average precision-recall curve and AUC\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(all_labels.ravel(), all_predictions.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(all_labels, all_predictions, average=\"micro\")\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(all_labels[:, i], all_predictions[:, i])\n",
    "    roc_auc[i] = roc_auc_score(all_labels[:, i], all_predictions[:, i])\n",
    "\n",
    "# Compute micro-average ROC curve and AUC\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(all_labels.ravel(), all_predictions.ravel())\n",
    "roc_auc[\"micro\"] = roc_auc_score(all_labels, all_predictions, average=\"micro\")\n",
    "\n",
    "# Split classes into groups of 10\n",
    "class_groups = [list(range(i, min(i + 10, num_classes))) for i in range(0, num_classes, 10)]\n",
    "\n",
    "# Plot Precision-Recall and ROC curves for each group\n",
    "for group in class_groups:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot Precision-Recall curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.step(recall['micro'], precision['micro'], where='post', label='Micro-average Precision-Recall curve (AUPR = {0:0.2f})'\n",
    "                 ''.format(average_precision[\"micro\"]))\n",
    "    for i in group:\n",
    "        plt.step(recall[i], precision[i], where='post', label='Precision-recall curve of class {0} (AUPR = {1:0.2f})'\n",
    "                 ''.format(i, average_precision[i]))\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall curve (Classes {})'.format(group))\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(fpr['micro'], tpr['micro'], label='Micro-average ROC curve (AUC = {0:0.2f})'\n",
    "                 ''.format(roc_auc[\"micro\"]))\n",
    "    for i in group:\n",
    "        plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (AUC = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve (Classes {})'.format(group))\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d890b37c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b446c428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nltk version is 3.5.\n",
      "The scikit-learn version is 1.4.1.post1.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "print('The nltk version is {}.'.format(nltk.__version__))\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357cbbdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
