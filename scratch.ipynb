{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297be033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def get_free_memory():\n",
    "    memory = psutil.virtual_memory()\n",
    "    return memory.available / (1024.0 ** 3)  # Convert bytes to gigabytes\n",
    "\n",
    "print(f\"Free CPU Memory: {get_free_memory():.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/home/toibazd/Most_frequent_IPs.json\", \"r\") as f:\n",
    "    ips = json.load(f)\n",
    "\n",
    "sorted_dict = sorted(ips.items(), key=lambda x: x[1], reverse=True)\n",
    "most_frequent_ips = [item[0] for item in sorted_dict[0:400]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700509fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(most_frequent_ips[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import csv\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ip_to_go = defaultdict(list)\n",
    "data_dict = defaultdict(list)\n",
    "enc = MultiLabelBinarizer()\n",
    "\n",
    "go = Ontology('data/go.obo')\n",
    "\n",
    "new_tsv_filename = \"/home/toibazd/Family_IPs_with_GO.tsv\"\n",
    "\n",
    "\n",
    "with open(new_tsv_filename, \"r\") as new_tsvfile:\n",
    "    reader = csv.reader(new_tsvfile, delimiter=\"\\t\")\n",
    "    next(reader)\n",
    "    for row in tqdm(reader):\n",
    "        ip = row[0]  # Assuming the IP is in the first column\n",
    "        go_terms = row[6]  # Assuming the GO terms are in the second column\n",
    "\n",
    "        # Add IP and corresponding GO terms to data_dict\n",
    "        ip_to_go[ip]+= go_terms.split(',')\n",
    "\n",
    "\n",
    "with open(\"/home/toibazd/Prot2IP_GO_filtered_MF.tsv\", \"r\") as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter = \"\\t\")\n",
    "    for row in tqdm(reader):\n",
    "        key = row[0].split(\"prot_\")[1].split(\".\")[0]\n",
    "        iprs = eval(row[1])\n",
    "        \n",
    "\n",
    "        # Save only if there are filtered InterPro IDs\n",
    "        for ip in iprs:\n",
    "            if ip in most_frequent_ips and ip_to_go[ip]:\n",
    "                for GO in ip_to_go[ip]:\n",
    "                    data_dict[key].append(GO)\n",
    "#                     data_dict[key].extend(list(go.get_ancestors(GO)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336bd750",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_values = [value for values in data_dict.values() for value in values]\n",
    "\n",
    "# Convert the list into a set to remove duplicates\n",
    "unique_go = set(all_values)\n",
    "\n",
    "print(\"Number of unique words:\", len(unique_go))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a48221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "enc = MultiLabelBinarizer()\n",
    "\n",
    "one_hot_encoded = enc.fit_transform(data_dict.values())\n",
    "one_hot_encoded_dict = {key: value for key, value in zip(data_dict.keys(), one_hot_encoded)}\n",
    "\n",
    "print(len(one_hot_encoded_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c77361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique numbers and their counts\n",
    "unique_numbers, counts = np.unique(one_hot_encoded, return_counts=True)\n",
    "all_count = 0\n",
    "# Print the count of each number\n",
    "for number, count in zip(unique_numbers, counts):\n",
    "    all_count+=count\n",
    "    print(f\"Number {number}: Count {count}\")\n",
    "print(all_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0995cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "directory = '/ibex/user/toibazd/InterPro_annotated_genomes_training/'\n",
    "one_hot_encoded_sentences = {}\n",
    "\n",
    "sentence_length = 40\n",
    "sentences_per_IP = 200\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Randomly choose 1000 files with seed 42\n",
    "selected_files = os.listdir(directory)\n",
    "\n",
    "\n",
    "# Define a function to process a file\n",
    "def process_file(filename, IP):\n",
    "    sentences = []\n",
    "\n",
    "    filepath = os.path.join(directory, filename)\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        content = file.read()\n",
    "        words = content.strip().split()\n",
    "\n",
    "        # Check if the key is in the file\n",
    "        for i in range(19, len(words)-20):\n",
    "            # Shuffle the indices of the words containing the key\n",
    "            if IP in data_dict[words[i]]:\n",
    "                if len(words) - i >= 21:\n",
    "                    sentence = \" \".join(words[i - 19:i + sentence_length - 19])\n",
    "                    sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Iterate over keys\n",
    "for IP in tqdm(unique_go):\n",
    "    one_hot_encoded_sentences[IP] = []\n",
    "    sentences_count = 0\n",
    "\n",
    "    # Use ThreadPoolExecutor for concurrent processing\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [executor.submit(process_file, filename, IP) for filename in selected_files]\n",
    "        for future in futures:\n",
    "            sentences = future.result()\n",
    "            one_hot_encoded_sentences[IP].extend(sentences)\n",
    "            sentences_count += len(sentences)\n",
    "            if sentences_count >= sentences_per_IP:\n",
    "                break\n",
    "\n",
    "    # Break if the required number of sentences per key is reached\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4474a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('BERT_DNN_senteces_with_GO.json', 'w') as f:\n",
    "    json.dump(one_hot_encoded_sentences, f)\n",
    "\n",
    "    \n",
    "print(\"Saved BERT_DNN_senteces_with_GO.json file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
