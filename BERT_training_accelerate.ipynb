{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abdd98ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-12 14:46:38,388] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 14:46:46.702394: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-12 14:46:51.171582: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-12 14:47:01.782280: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /sw/rl9g/nccl/2.17.1/cuda11.8/lib:/sw/rl9g/cuda/11.8/rl9_binary/lib64:/sw/rl9g/cuda/11.8/rl9_binary/lib:/sw/rl9g/cuda/11.8/rl9_binary/lib/stubs\n",
      "2024-03-12 14:47:01.783596: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /sw/rl9g/nccl/2.17.1/cuda11.8/lib:/sw/rl9g/cuda/11.8/rl9_binary/lib64:/sw/rl9g/cuda/11.8/rl9_binary/lib:/sw/rl9g/cuda/11.8/rl9_binary/lib/stubs\n",
      "2024-03-12 14:47:01.783608: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from pynvml import *\n",
    "import psutil\n",
    "from accelerate import Accelerator\n",
    "from datasets import Dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from transformers import AutoModelForMaskedLM, BertConfig, get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47ff8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    accelerator = Accelerator() \n",
    "\n",
    "\n",
    "    wandb.login(key = \"5c0f1505d0af16a0dda3f3d031310d45e9a3f07b\")\n",
    "\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "    tokenizer_path = \"WordLevel_tokenizer_trained_InterPro.json\"\n",
    "    tokenizer = tokenizer.from_file(tokenizer_path)\n",
    "    tokenizer.enable_truncation(512)\n",
    "    train_dataset = Dataset.load_from_disk('BERT_train_dataset')\n",
    "    val_dataset = Dataset.load_from_disk('BERT_val_dataset')\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
    "    accelerator.print(len(train_dataloader))\n",
    "    config = BertConfig(vocab_size = tokenizer.get_vocab_size(), hidden_size = 256, num_hidden_layers = 3, num_attention_heads = 8, intermediate_size = 256)\n",
    "    model = AutoModelForMaskedLM.from_config(config)\n",
    "\n",
    "    epochs = 5\n",
    "    optimizer = optim.AdamW(model.parameters(),lr=1e-3, weight_decay=2e-5)\n",
    "\n",
    "    num_training_steps = epochs * len(train_dataloader) \n",
    "    num_warmup_steps = int(num_training_steps*0.05)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "    def print_gpu_utilization():\n",
    "        nvmlInit()\n",
    "        handle = nvmlDeviceGetHandleByIndex(0)\n",
    "        info = nvmlDeviceGetMemoryInfo(handle)\n",
    "        accelerator.print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "    # Function to get free CPU memory\n",
    "    def get_free_memory():\n",
    "        memory = psutil.virtual_memory()\n",
    "        return memory.available / (1024.0 ** 3)  # Convert bytes to gigabytes\n",
    "\n",
    "    # Display free CPU memory\n",
    "    accelerator.print(f\"Free CPU Memory: {get_free_memory():.2f} GB\")\n",
    "    wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"InterPro_BERT_training_final\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"architecture\": \"BERT\",\n",
    "    \"dataset\": \"InterPro_genomes\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    "    )\n",
    "    accelerator.print(\"LOADING MODEL\")\n",
    "    model, optimizer, scheduler, train_dataloader = accelerator.prepare(model,optimizer, scheduler, train_dataloader)\n",
    "    accelerator.print(len(train_dataloader))\n",
    "    accelerator.print(\"NOW WILL START TRAINING\")\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    val_acc = []\n",
    "\n",
    "    # best_val_loss = float('inf')  \n",
    "    # patience = 3 \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_correct = 0\n",
    "        total_tokens = 0\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        model.train()\n",
    "        accelerator.print(f\"training epoch {epoch}\")\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "\n",
    "            outputs = model(input_ids,attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            train_loss+=loss.item()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if step%500==0:\n",
    "                wandb.log({\"train_loss\": loss.item()})\n",
    "        print_gpu_utilization()\n",
    "        accelerator.print(f\"evaluation epoch {epoch}\")\n",
    "        model.eval()\n",
    "        count=0\n",
    "        for step, batch in enumerate(val_dataloader):\n",
    "            input_ids = batch['input_ids'].to(accelerator.device)\n",
    "            attention_mask = batch['attention_mask'].to(accelerator.device)\n",
    "            labels = batch['labels'].to(accelerator.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            # Mask out labels where input_ids != 4\n",
    "            mask = (input_ids == 4)\n",
    "            masked_labels = labels[mask]\n",
    "            masked_predicted_labels = predicted_labels[mask]\n",
    "\n",
    "            correct = torch.sum(masked_predicted_labels == masked_labels).item()\n",
    "            total_correct += correct\n",
    "            total_tokens += masked_labels.numel()\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            if step%500==0:\n",
    "                wandb.log({\"val_loss\":loss.item(), \"val_acc\":correct/masked_labels.numel()})\n",
    "        accuracy = total_correct / total_tokens if total_tokens > 0 else 0.0\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "        training_loss.append(avg_train_loss)\n",
    "        validation_loss.append(avg_val_loss)\n",
    "\n",
    "        accelerator.print(\"Train loss:\", avg_train_loss)\n",
    "        accelerator.print(\"Val loss:\", avg_val_loss)\n",
    "        accelerator.print(\"Acc: \", accuracy )\n",
    "        accelerator.print(\"\\n\\n\")\n",
    "\n",
    "    #     if avg_val_loss < best_val_loss:\n",
    "    #         best_val_loss = avg_val_loss\n",
    "    #         torch.save(model.state_dict(), '/BERT_context_pretrained_10K/BERT_best.pth')  # Save the best model\n",
    "\n",
    "    #     else:\n",
    "    #         patience -=1\n",
    "    #         if patience== 0:\n",
    "    #             # Stop training if validation loss doesn't improve after patience epochs\n",
    "    #             print(f\"Stopping early as validation loss didn't improve for {patience} epochs.\")\n",
    "    #             break  # Break out of the training loop\n",
    "\n",
    "\n",
    "    wandb.finish()\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        'BERT_context_pretrained_InterPro_final',\n",
    "        is_main_process=accelerator.is_main_process,\n",
    "        save_function=accelerator.save)\n",
    "    print(\"Saved pre_trained model here: BERT_context_pretrained_InterPro_final\")\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        plt.plot(list(range(len(training_loss))), training_loss, linestyle='dotted', label='Training Loss')\n",
    "        plt.plot(list(range(len(validation_loss))), validation_loss, marker='o', linestyle='solid', label='Validation Loss')\n",
    "\n",
    "        plt.title('Training and Validation Loss Over Epochs')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "\n",
    "        plt.savefig('loss_plot_final.png', dpi=300) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9282be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93cb03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
